---
title: "Project Two"
author: "Benjamin Pope"
date: "3/8/2021"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE}
#Library
library(e1071)
library(klaR)
library(nnet)
library(neuralnet)
library(MASS)
library(rpart)
library(randomForest)
library(mlbench) #Includes BreastCancer data set. 
library(caret)
library(stringr)
```

```{r Import data}
#Import data
data("BreastCancer")

# Set up columns correctly. 
# Set label as the first column in data frame.
mydata <- cbind(BreastCancer[11],BreastCancer[2:10])
```

```{r Data Clean-up}
#Look at the data to see its structure and if there are any missing values. 
summary(mydata)
str(mydata)

#Impute missing values with mean
for (i in 1:ncol(mydata)) {
  mydata[is.na(mydata[,i]), i] <- floor(mean(as.numeric(mydata[,i]), na.rm = TRUE))
}
mydata$Malignant_1 <- ifelse(mydata$Class == "malignant",1,0)  #Set our label to binary for our numerical data frame

mydata.num <- as.data.frame(apply(mydata[,2:11],2,as.numeric))
mydata.num <- cbind(mydata.num[10],mydata.num[1:9]) # Reorder our numerical data frame so that our label is first
mydata <- mydata[,1:10] #Removes our numerical label from our factored data frame 

```


```{r Split train and valid sets.}
#Partition Data
t_index <- sample(c(1:dim(mydata)[1]), dim(mydata)[1]*.6) # Sets our training data to 60% of our data
train.df <- mydata[t_index, ]
valid.df <- mydata[-t_index, ]

```

```{r}
#set up data frame for collecting accuracy
accuracy.df <- data.frame(Model = seq(1, 8,1), Train_Accuracy_score = rep(0,8) ,Valid_Accuracy_score = rep(0,8))
```


```{r Support Vector Machines }
# Support Vector machines
accuracy.df[1,1] <- "Support Vector Machines"
bdsvm <- svm(Class~.,train.df)
bdsvm.pred <- predict(bdsvm,train.df)
accuracy.df[1,2] <-confusionMatrix(as.factor(bdsvm.pred), as.factor(train.df$Class))$overall[1]
#Create Predictions
bdsvm.v.pred <- predict(bdsvm, valid.df)
#put in accuracy to data frame
accuracy.df[1,3] <- confusionMatrix(as.factor(bdsvm.v.pred), as.factor(valid.df$Class))$overall[1]

```

```{r Naive Bays }
#Naive Bays
accuracy.df[2,1] <- "Naive Bays"
bdnvb <-NaiveBayes(Class ~., train.df)
bdnvb.pred <- predict(bdnvb, train.df)
accuracy.df[2,2] <- confusionMatrix(as.factor(bdnvb.pred$class), as.factor(train.df$Class))$overall[1]
#Create Predictions
bdnvb.v.pred <- predict(bdnvb, valid.df)
#Put in accuracy into data frame
accuracy.df[2,3] <- confusionMatrix(as.factor(bdnvb.v.pred$class), as.factor(valid.df$Class))$overall[1]
```

```{r Neural Net setup}
#Neural Net
#Set our training and validation datasets.  *NOTE be sure to use our numerical data frames for NN 
train.num.df <- mydata.num[t_index, ]
valid.num.df <- mydata.num[-t_index, ]

#normalize training and valid sets.
norm_values <- preProcess(train.num.df[,2:10])
train.norm.df <- predict(norm_values, train.num.df)
valid.norm.df <- predict(norm_values, valid.num.df)
```

```{r Running Neural Net}
#Create Neural Network
accuracy.df[3,1] <- "Neural Net"
bdnnet <- neuralnet(Malignant_1 ~ .,linear.output = T, data = train.norm.df, hidden = c(2,5), rep = 5) #NN of 2 layers with 5 nodes.  
train.pred <- compute(bdnnet, train.norm.df)
train.class <- ifelse(train.pred$net.result > .5, 1, 0)
accuracy.df[3,2] <- confusionMatrix(as.factor(train.class), as.factor(train.num.df$Malignant_1))$overall[1]
#create Predictions
valid.pred <- compute(bdnnet, valid.norm.df)
valid.class <- ifelse(valid.pred$net.result > .5, 1, 0)
#put in accuracy to data frame.
accuracy.df[3,3] <- confusionMatrix(as.factor(valid.class), as.factor(valid.norm.df$Malignant_1))$overall[1]

```

```{r Decision Trees}
#Decision trees
accuracy.df[4,1] <- "Decision Tree"
bdtree <- rpart(Class~ ., train.df)
#create predictions for DT
bdtree.pred <- predict(bdtree, train.df, type = "class")
bdtree.v.pred <- predict(bdtree, valid.df, type = "class")
#put in accuracy to data frame.
accuracy.df[4,2] <- confusionMatrix(as.factor(bdtree.pred), as.factor(train.df$Class))$overall[1]

accuracy.df[4,3] <- confusionMatrix(bdtree.v.pred, valid.df$Class)$overall[1]

```

```{r Cross Validation }
# Leave-1-Out Cross Validation (LOOCV)

accuracy.df[5,1] <- "Leave-1-Out Cross Validation (LOOCV)"

ans <- numeric(length(as.numeric(valid.df[,1])))
for (i in 1:length(valid.df[,1])) {
  bdtree2 <- rpart(Class ~ ., valid.df[-i,])
  bdtree2.pred <- predict(bdtree, valid.df[i,],type="class")
  ans[i] <- bdtree2.pred
}
ans <- factor(ans,labels=levels(valid.df$Class))

accuracy.df[5,3] <- confusionMatrix(as.factor(ans), as.factor(valid.df$Class))$overall[1]
# The same as above in this case
```

```{r Regularized Discriminant Analysis } 
accuracy.df[6,1] <- "Regularized Discriminant Analysis"
bdrda <- rda(Class ~ ., train.df)
bdrda.pred <- predict(bdrda, train.df)
accuracy.df[6,2] <- confusionMatrix(as.factor(bdrda.pred$class), as.factor(train.df$Class))$overall[1]

bdrda2 <- rda(Class ~ ., mydata)
bdrda.v.pred <- predict(bdrda, valid.df)
accuracy.df[6,3] <- confusionMatrix(as.factor(bdrda.v.pred$class), as.factor(valid.df$Class))$overall[1]
```

```{r Random Forests }
#Random Forests
accuracy.df[7,1] <- "Random Forests"
bdrf <- randomForest(Class~., train.df, importance= TRUE)
bdrf.pred <- predict(bdrf, train.df)
accuracy.df[7,2] <- confusionMatrix(bdrf.pred, train.df$Class)$overall[1]
# (Suspiciously correct! - need to read the manual)

bdrf.v.pred <- predict(bdrf, valid.df)
accuracy.df[7,3] <- confusionMatrix(as.factor(bdrf.v.pred), as.factor(valid.df$Class))$overall[1]

```

```{r Ensamble model }
#Pull in results from three models


ensamble.df <- cbind(as.data.frame(bdsvm.v.pred)[1],as.data.frame(bdnvb.v.pred)[1],as.data.frame(valid.class)[1],as.data.frame(bdtree.v.pred)[1],as.data.frame(ans),as.data.frame(bdrda.v.pred)[1],as.data.frame(bdrf.v.pred)[1])
colnames(ensamble.df) <-c("svm", "nvb", "nnet","dectree","LOOCV", "rda", "rf" )

ensamble.df$svm <- ifelse(ensamble.df$svm == "malignant",1,0)
ensamble.df$nvb <- ifelse(ensamble.df$nvb == "malignant",1,0)
ensamble.df$dectree <- ifelse(ensamble.df$dectree =="malignant",1,0)
ensamble.df$rda <- ifelse(ensamble.df$rda == "malignant",1,0)
ensamble.df$LOOCV <- ifelse(ensamble.df$LOOCV =="malignant",1,0)
ensamble.df$rf <- ifelse(ensamble.df$rf =="malignant",1,0)
#Use sum answer as output
e <- as.matrix(ensamble.df) #matrix for rowsums
#put our sums back onto the ensamble data frame. 
ensamble.df$combo <- rowSums(e)

#Give us a class based answer again
ensamble.df$combo_class <- ifelse(ensamble.df$combo >=4,"malignant", "benign")
#Check our accuracy with our ensamble score 
accuracy.df[8,1] <- "Combo Score "
accuracy.df[8,3] <- confusionMatrix(as.factor(ensamble.df$combo_class), as.factor(valid.df$Class))$overall[1]
```

```{r}
#Here is all of our models accuaracies.  We can take these results to our deployment when we decidee on which models to use.  
accuracy.df
```